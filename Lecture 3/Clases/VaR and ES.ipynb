{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\")) #$\\color{red}{\\text{ciao}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python para Finanzas y Ciencia de Datos\n",
    "____\n",
    "\n",
    "1. Statistics Concepts\n",
    "2. Portfolio Optimization\n",
    "3. Principal Component Analysis (PCA)\n",
    "4. Value-at-Risk (VaR)\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTAMOS LAS LIBRERÍAS QUE VAMOS A USAR A LO LARGO DEL MÓDULO.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_datareader.data as reader\n",
    "\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from numpy.random import multivariate_normal\n",
    "import yfinance as yf\n",
    "\n",
    "import scipy.stats as scs\n",
    "import scipy.optimize as sco\n",
    "import scipy.interpolate as sci\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "#import functions as f \n",
    "\n",
    "import random\n",
    "random.seed(1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Risk measures\n",
    "---\n",
    "Consider three different type of assets, all with the same volatility and mean, but different distributions (e.g.: normal, uniform, Student-t). Standard mean variance analysis indicates that all three assets are equally risky and preferable because their means and volatilities are the same. However, in reality, market participants view the risk in them quite differently and are likely to have a preference for one of these assets. This suggests that some objective way to compare the risk in assets with different distribution is desirable.\n",
    "\n",
    "Volatility or **standard deviation** is the main measure of risk in most financial analysis, but it **is sufficient as a risk measure only when returns are normally distributed**. However, as discused before, the normality assumption is **violeted for most if not all financial returns**. \n",
    "\n",
    "\n",
    "## Value-at-risk (VaR)\n",
    "\n",
    "The most common risk measure after volatility is **value-at-risk (VaR)**. It is a single summary statistical measure of risk, it is distribution independent and it is a measure of losses as a result of \"typical\" market movements.\n",
    "\n",
    "VaR has become one of the most widely used risk measures, and a much debated one. Loved by practitioners for its intuitive appeal, it is also discussed and criticized by many - mainly on theoretical grounds, with regard to its limited ability to capture what is called *tail risk*.\n",
    "\n",
    "#### Definition: \n",
    "***The loss on a trading portfolio such that there is a probability $p$ of losses equaling or exceeding VaR in a given trading period and a $(1-p)$ probability of losses being lower than the VaR (i.e., the confidence level).***\n",
    "\n",
    "We may write it as $VaR(p)$ or $VaR$ at $100 \\times  p \\%$ to make the dependence on probability explicit. The most common probability levels are 1% or 5%.\n",
    "\n",
    "VaR is a quantile of the distribution of profit and loss (P&L). We indicate P&L on an investment portfolio by the random variable $Q$, with a particular realization indicated by $q$. The density of P/L is denoted by $f_q (.)$. Thus, VaR is given by:\n",
    "\n",
    "$$p = Pr[Q \\leq -VaR(p)]$$\n",
    "\n",
    "or\n",
    "\n",
    "$$p = \\int_{-\\infty}^{-VaR(p)}f_q(x)dx$$\n",
    "\n",
    "We use a minus sign because VaR is a positive number and we are dealing with losses. You should then interpret it as *the probability of losses being larger than VaR*.\n",
    "\n",
    "\n",
    "#### Example:\n",
    "\n",
    "Consider a stock position, worth 1 million USD today, that has a $VaR(0.05)$ (or $VaR$ at $5\\%$) of 10000 USD over a time period of 30 days (one month). This VaR says that the loss to be expected over a period of 30 days can exceed 50,000 USD with probability 0.01. In other words, there is a probability of 0.99 that losses will not exceed that value. \n",
    "However, it does not say anything about the size of the loss once a loss beyond 50,000 USD occurs—i.e., if the maximum loss is 100,000 or 500,000 USD what the probability of such a specific “higher than VaR loss” is. That is what is called the tail risk.\n",
    "\n",
    "<img src='https://upload.wikimedia.org/wikipedia/commons/6/64/VaR_diagram.JPG' width=\"60%\" align=\"center\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can study VaR via two methods:\n",
    "\n",
    "1. Historical distribution\n",
    "2. Monte Carlo simulations\n",
    "\n",
    "\n",
    "## 1. Historical distribution \n",
    "\n",
    "The issue here is: what is the probability distibution of P&L of a portfolio? The above discussion was based on the assumption that that distribution was known. Howerever, in practice, one needs to estimate the P&L distribution using historical observations of the asset returns of interest. This historical simulation is a simple method for forecasting risk and relies on the assumption that history repeats itself, where one of the observed past returns is expected to be the next period return.\n",
    "\n",
    "Let's take some of the fund returns we calculated above and calcule the VaR. \n",
    "Suppose we invest 10,000 for 22 days. We are interested in the VaR of a financial position in the S&P 500 and the iShares US Treas 10y-20y fund for 22 days.\n",
    "\n",
    "We use **scs.scoreatpercentile** to get the VaR. **scoreatpercentile(a)** calculates the score at a given percentile of the input sequence **a**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_investment = 10000\n",
    "investment_horizon = 22\n",
    "SP500_prices = data['S&P 500']\n",
    "US_treas_prices= data['iShares US Treas 10y-20y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SP500_prices1 = SP500_prices[:-investment_horizon]\n",
    "US_treas_prices1 = US_treas_prices[:-investment_horizon]\n",
    "\n",
    "SP500_prices2 = SP500_prices[investment_horizon:]\n",
    "US_treas_prices2 = US_treas_prices[investment_horizon:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SP500 = (SP500_prices2.values/SP500_prices1.values - 1)*initial_investment\n",
    "US_treas = (US_treas_prices2.values/US_treas_prices1.values -1)*initial_investment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SP500 = pd.Series(SP500, name = 'S&P 500')\n",
    "US_treas = pd.Series(US_treas, name = 'US Treas 10y-20y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_var(portfolio, p):\n",
    "    '''\n",
    "    Esta función grafica la distribución de P&L y encuentra el VaR para\n",
    "    un nivel p%.\n",
    "    Además, \n",
    "    '''\n",
    "    VaR = scs.scoreatpercentile(portfolio, p)\n",
    "    \n",
    "    fig = plt.figure(figsize = (18,7))\n",
    "     \n",
    "    plt.hist(portfolio, bins = 60)\n",
    "    plt.xlabel('P&L')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    plt.axvline(VaR, color='r', linestyle='dashed', linewidth=2, \n",
    "                label = 'VaR at '+str(p)+'% =' +str(round(VaR,2)))\n",
    "    plt.text(VaR- np.std(portfolio), 100, \n",
    "             'p = '+str(p/100), fontsize = 12)\n",
    "    plt.title('Distribution of '+ portfolio.name)\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc='best', fontsize = 12)\n",
    "    ;\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_var(SP500, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_var(US_treas, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also find the VaR for many confidence levels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_at_risk(portfolio):\n",
    "    \n",
    "    percs = [0.01, 0.1, 1., 2.5, 5.0, 10.0]\n",
    "    VaR = scs.scoreatpercentile(portfolio, percs)\n",
    "    print(\"%16s %16s\" % ('Confidence Level', 'Value-at-Risk'))\n",
    "    print(33 * \"-\")\n",
    "    for pair in zip(percs, -VaR):\n",
    "        print(\"%11.2f %17.3f\" % (100-pair[0], -pair[1]))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_at_risk(SP500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_at_risk(US_treas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percs = np.linspace(0,15)\n",
    "SP500_var = scs.scoreatpercentile(SP500, percs)\n",
    "US_treas_var = scs.scoreatpercentile(US_treas, percs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (18,6))\n",
    "plt.plot(percs, -SP500_var, lw=2, label='S&P 500', c='r')\n",
    "plt.plot(percs, -US_treas_var, lw=2, label='US Treasury', c='b')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('P (%)')\n",
    "plt.ylabel('Value-at-risk')\n",
    "plt.title('Value-at-risk')\n",
    "plt.grid(True)\n",
    "plt.autoscale(enable=True, axis='x', tight=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each $p$, VaR of the S&P is always higher than VaR of US Treasuries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Monte Carlo simulations\n",
    "\n",
    "Given the limitations of the above methods, a good replacement for VaR analysis is to perform Monte Carlo (MC) simulations.\n",
    "The idea behind MC simulation is that we replicate market outcomes on the computer, based on some model of the evolution of the market. By doing a sufficient number of simulations, we get a large sample of market outcomes enabling us to calculate accurately some quantities of interests (VaR in this case). The main limitation of this approach is that it is always based on some model and the quality of the results in inevitably limited by the quality of the model.\n",
    "\n",
    "For ilustration purposes, let's generate 3 different returns distributions and see how VaR looks like for each one. The first two -standard normal and Student-t - are the most common one for replicating returns whereas the third one -uniform distributed- is an unusual case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Normal\n",
    "norm = np.random.standard_normal(10000)\n",
    "\n",
    "# Student t\n",
    "t = np.random.standard_t(5,10000)\n",
    "\n",
    "#Uniform\n",
    "unif = np.random.uniform(-1,1,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (22,6))\n",
    "plt.subplot(1,3,1)\n",
    "plt.hist(norm, color = 'blue', bins = 50)\n",
    "plt.title('Standard normal')\n",
    "plt.axvline(scs.scoreatpercentile(norm, 5), color='r', linestyle='dashed', linewidth=3)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.hist(t, color = 'orange', bins = 50)\n",
    "plt.title('Student-t')\n",
    "plt.axvline(scs.scoreatpercentile(t, 5), color='r', linestyle='dashed', linewidth=3)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.hist(unif, color = 'green', bins = 50)\n",
    "plt.title('Uniform')\n",
    "plt.axvline(scs.scoreatpercentile(unif, 5), color='r', linestyle='dashed', linewidth=3)\n",
    "plt.grid(True)\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now generate daily returns of 3 assets, with the same Stundent-t distribution but with different degrees of freedom (df). As the degrees of freedom increase, fewer extreme observations are obtained.\n",
    "\n",
    "If df $\\rightarrow \\infty$, Student-t $\\rightarrow$ Normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rets1 = np.random.standard_t(5,10000)*22/252\n",
    "rets2 = np.random.standard_t(50,10000)*22/252\n",
    "rets3 = np.random.standard_t(100,10000)*22/252\n",
    "rets = [rets1, rets2, rets3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a portfolio and calculate the VaR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [1/3, 1/3, 1/3]\n",
    "port_rets = np.dot(weights, rets)\n",
    "port_rets = pd.Series(port_rets * initial_investment, name ='simulated portfolio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_var(port_rets, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_at_risk(port_rets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected shortfall (ES)\n",
    "\n",
    "One of the flaws of the VaR aproach is that it is simply a quantile. In practice, the actual loss, if it occurs, can be grater than VaR. In this sense, VaR may understimate the actual loss.\n",
    "The most common alternative risk measure that tries to fill that gap is **expected shortfall** (ES), also known as tail VaR, conditional Value at Risk (CVaR), among others. ES answers the question: *what is expected loss when losses exceed VaR?*\n",
    "\n",
    "####  Definition: \n",
    "*Expected loss conditional on VaR being violated. $ES = -E[Q|Q\\leq-VaR(p)]$*\n",
    "\n",
    "\n",
    "Let's define a function for ES and add it to our previous plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ES(portfolio, p):\n",
    "    VaR = scs.scoreatpercentile(portfolio, p)\n",
    "    shortfall = portfolio[portfolio<VaR]\n",
    "        \n",
    "    return shortfall.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask1 = port_rets < scs.scoreatpercentile(port_rets, 5)\n",
    "10000/len(port_rets[mask1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_var_ES(portfolio, p):\n",
    "    \n",
    "    VaR = scs.scoreatpercentile(portfolio, p)\n",
    "    ExpS = ES(portfolio, p)\n",
    "    \n",
    "    fig = plt.figure(figsize = (18,7))\n",
    "    \n",
    "    mask1 = portfolio < VaR\n",
    "    mask2 = portfolio> VaR\n",
    "    \n",
    "    plt.hist(portfolio[mask2], bins = 60)\n",
    "    plt.hist(portfolio[mask1], bins = 25, color='red')\n",
    "\n",
    "    plt.axvline(VaR, color='r', linestyle='dashed', linewidth=2, \n",
    "                label = 'VaR at 5% ='+ str(round(VaR,2)))\n",
    "    plt.axvline(ExpS, color='green', linestyle='dashdot', linewidth=2, \n",
    "                label = 'Expected shortfall = '+str(round(ES(portfolio,p),2)))\n",
    "    \n",
    "    plt.title('Distribution of '+ portfolio.name)\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc='best', fontsize = 12)\n",
    "    plt.xlabel('P&L')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xlim((-3000, 3000))\n",
    "    ;\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_var_ES(SP500, 5)\n",
    "plot_var_ES(US_treas, 5)\n",
    "plot_var_ES(port_rets, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
